{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Define language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading files and detecting given languages manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading 2.tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag End先生的Bilbo Baggins先生宣布，他不久將以特別的輝煌的聚會慶祝他的第...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Bag End先生的Bilbo Baggins先生宣布，他不久將以特別的輝煌的聚會慶祝他的第..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1 = pd.read_csv('2.tmp', names=['text'], sep='\\n').dropna()\n",
    "dataset1[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1['lang'] = 'chinese'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading 3.tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When Mr. Bilbo Baggins of Bag End announced th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  When Mr. Bilbo Baggins of Bag End announced th..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2 = pd.read_csv('3.tmp', names=['text'], sep='\\n').dropna()\n",
    "dataset2[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2['lang'] = 'english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading 4.tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag EndのBilbo Baggins氏は、18歳の誕生日を特別な壮大なパーティでまもな...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Bag EndのBilbo Baggins氏は、18歳の誕生日を特別な壮大なパーティでまもな..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3 = pd.read_csv('4.tmp', names=['text'], sep='\\n').dropna()\n",
    "dataset3[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3['lang'] = 'japanese'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat((dataset1, dataset2, dataset3))\n",
    "dataset.index = np.arange(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding dataset representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag End先生的Bilbo Baggins先生宣布，他不久將以特別的輝煌的聚會慶祝他的第...</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>什麼，人們普遍相信，Bag End山充滿了寶藏的隧道。如果這還不夠名聲，那麼他也有長期的活力...</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>青年以及（著名地）無窮無盡的財富似乎是不公平的。他們說：“這將需要支付。” “這不是自然的，...</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>仍然與親屬（除了當然是薩克維爾 - 巴金斯）的訪問條件，他在貧窮和不重要的家庭的愛好中有許多...</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>＃＃”你說得對，爸爸！“Gaffer說。 “不是巴剋土地的白蘭地酒在老森林裡生活，但他們似乎...</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     lang\n",
       "0  Bag End先生的Bilbo Baggins先生宣布，他不久將以特別的輝煌的聚會慶祝他的第...  chinese\n",
       "1  什麼，人們普遍相信，Bag End山充滿了寶藏的隧道。如果這還不夠名聲，那麼他也有長期的活力...  chinese\n",
       "2  青年以及（著名地）無窮無盡的財富似乎是不公平的。他們說：“這將需要支付。” “這不是自然的，...  chinese\n",
       "3  仍然與親屬（除了當然是薩克維爾 - 巴金斯）的訪問條件，他在貧窮和不重要的家庭的愛好中有許多...  chinese\n",
       "4  ＃＃”你說得對，爸爸！“Gaffer說。 “不是巴剋土地的白蘭地酒在老森林裡生活，但他們似乎...  chinese"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding hex representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x65e5'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(ord('日'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then implementing classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifySymbol(c):\n",
    "    # 0x0020—0x007F, 0x2000—0x206F - english\n",
    "    def isEnglish(c):\n",
    "        return int('0x0020', base=16) <= ord(c) <= int('0x007F', base=16) or \\\n",
    "               int('0x2000', base=16) <= ord(c) <= int('0x206F', base=16)\n",
    "    \n",
    "    # 0x3040—0x30FF - japanese\n",
    "    def isJapanese(c):\n",
    "        return int('0x3040', base=16) <= ord(c) <= int('0x30FF', base=16)\n",
    "    \n",
    "    # 0x4E00—0x9FFF - chineese\n",
    "    def isChinese(c):\n",
    "        return int('0x4E00', base=16) <= ord(c) <= int('0x9FFF', base=16)\n",
    "\n",
    "    \n",
    "    if isEnglish(c):\n",
    "        return 'english'\n",
    "    elif isJapanese(c):\n",
    "        return 'japanese'\n",
    "    elif isChinese(c):\n",
    "        return 'chinese'\n",
    "    else:\n",
    "        return 'undef'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyString(line):\n",
    "    r = dict({'english': 0, 'japanese': 0, 'chinese': 0, 'undef': 0})\n",
    "    for c in line:\n",
    "        r[classifySymbol(c)] += 1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chinese': 9, 'english': 6, 'japanese': 8, 'undef': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifyString('lalaka日別的輝煌的聚壮大なパーティでまも')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using KFold because required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bag End先生的Bilbo Baggins先生宣布，他不久將以特別的輝煌的聚會慶祝他的第...'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][:1].to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9903474903474904\n",
      "0.987146529562982\n",
      "0.9897039897039897\n",
      "0.9897172236503856\n",
      "0.9903474903474904\n",
      "0.987146529562982\n",
      "0.9897106109324759\n",
      "0.9896907216494846\n",
      "0.9884244372990354\n",
      "0.9948453608247423\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(dataset):\n",
    "    \n",
    "    train_texts = np.array(dataset['text'])[train_index]\n",
    "    test_texts = np.array(dataset['text'])[test_index]\n",
    "\n",
    "    true_train = np.array(dataset['lang'])[train_index]\n",
    "    true_test = np.array(dataset['lang'])[test_index]\n",
    "    \n",
    "    pred_train = [max(d, key=d.get) for d in [classifyString(s) for s in train_texts]]\n",
    "    pred_test = [max(d, key=d.get) for d in [classifyString(s) for s in test_texts]]\n",
    "    \n",
    "    print(accuracy_score(true_train, pred_train))\n",
    "    print(accuracy_score(true_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to modify classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Управляющие символы(0x0000-0x001F). В нашем случае их всего два: нуль-символ и символ перевода строки. Они не влияют на классификацию, поэтому их можно не учитывать.\n",
    "2. Основная латинница(0x0020—0x007F). Содержит также графические знаки. В нашем датасете соответствует лишь английскому языку, однако арабские цифры (в силу присутствия их во всех текстах) мы будем относить ко всем трем языкам.\n",
    "3. Знаки пунктуации(0x2000—0x206F). Будем относить к английскому языку, так как для языков восточной группы существуют свои знаки препинания.\n",
    "4. Символы и пунктуация ККЯ(0x3000—0x303F). Используются в китайском и японском языках.\n",
    "5. Катакана и хирагана(0x3040—0x30FF). Используются в японском языке.\n",
    "6. Унифицированные иероглифы ККЯ(0x4E00—0x9FFF). Используются в основном в китайском языке.\n",
    "7. Полуширинные и полноширинные формы(0xFF00—0xFFEF). Используются в китайском и японском языках.\n",
    "\n",
    "#### @copyright \n",
    "#### Источник классификации символов: Сергей Винокуров. (заимствован только текст этой клетки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifySymbol(c):\n",
    "    def isEnglish(c):\n",
    "        return 0x0020 <= ord(c) <= 0x007F or \\\n",
    "               0x2000 <= ord(c) <= 0x206F\n",
    "        \n",
    "    \n",
    "    def isJapanese(c):\n",
    "        return 0x3040 <= ord(c) <= 0x30FF or \\\n",
    "               0x3000 <= ord(c) <= 0x303F or \\\n",
    "               0xFF00 <= ord(c) <= 0xFFEF\n",
    "    \n",
    "    def isChinese(c):\n",
    "        return 0x4E00 <= ord(c) <= 0x9FFF or \\\n",
    "               0x3000 <= ord(c) <= 0x303F or \\\n",
    "               0xFF00 <= ord(c) <= 0xFFEF\n",
    "\n",
    "    res = dict({'english': 0, 'japanese': 0, 'chinese': 0, 'undef': 0})\n",
    "    \n",
    "    if isEnglish(c):\n",
    "        res['english'] = 1\n",
    "    elif isJapanese(c):\n",
    "        res['japanese'] = 1\n",
    "    elif isChinese(c):\n",
    "        res['chinese'] = 1\n",
    "    else:\n",
    "        res['undef'] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyString(line):\n",
    "    r = dict({'english': 0, 'japanese': 0, 'chinese': 0, 'undef': 0})\n",
    "    for c in line:\n",
    "        for k,v in classifySymbol(c).items(): \n",
    "            r[k] += v\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chinese': 0, 'english': 8, 'japanese': 0, 'undef': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifyString('asdfasdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9916344916344917\n",
      "0.9948586118251928\n",
      "0.9929214929214929\n",
      "0.9897172236503856\n",
      "0.990990990990991\n",
      "0.9974293059125964\n",
      "0.9929260450160772\n",
      "0.9896907216494846\n",
      "0.9929260450160772\n",
      "0.9896907216494846\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(dataset):\n",
    "    \n",
    "    train_texts = np.array(dataset['text'])[train_index]\n",
    "    test_texts = np.array(dataset['text'])[test_index]\n",
    "\n",
    "    true_train = np.array(dataset['lang'])[train_index]\n",
    "    true_test = np.array(dataset['lang'])[test_index]\n",
    "    \n",
    "    pred_train = [max(d, key=d.get) for d in [classifyString(s) for s in train_texts]]\n",
    "    pred_test = [max(d, key=d.get) for d in [classifyString(s) for s in test_texts]]\n",
    "    \n",
    "    print(accuracy_score(true_train, pred_train))\n",
    "    print(accuracy_score(true_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nice results. Let's try something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "def classifyString_1(line):\n",
    "    res = dict()\n",
    "    try:\n",
    "        key = detect(line)[:2]\n",
    "    except:\n",
    "        return 'english'\n",
    "    \n",
    "    if (key == 'zh'):\n",
    "        key = 'chinese'\n",
    "    elif (key == 'ja'):\n",
    "        key = 'japanese'\n",
    "    else:\n",
    "        key = 'english'\n",
    "    \n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9626769626769627\n",
      "0.9485861182519281\n",
      "0.9658944658944659\n",
      "0.9537275064267352\n",
      "0.9613899613899614\n",
      "0.9588688946015425\n",
      "0.9588424437299036\n",
      "0.9510309278350515\n",
      "0.9594855305466238\n",
      "0.9664948453608248\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(dataset):\n",
    "    \n",
    "    train_texts = np.array(dataset['text'])[train_index]\n",
    "    test_texts = np.array(dataset['text'])[test_index]\n",
    "\n",
    "    true_train = np.array(dataset['lang'])[train_index]\n",
    "    true_test = np.array(dataset['lang'])[test_index]\n",
    "    \n",
    "    pred_train = [classifyString_1(s) for s in train_texts]\n",
    "    pred_test = [classifyString_1(s) for s in test_texts]\n",
    "    \n",
    "    print(accuracy_score(true_train, pred_train))\n",
    "    print(accuracy_score(true_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quite nice result too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
